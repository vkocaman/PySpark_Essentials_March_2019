{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark - From Zero to Hero (March 2019)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of PySpark\n",
    "\n",
    "- Apache Spark is written in Scala\n",
    "\n",
    "- To support Python with Spark, Apache Spark Community released PySpark\n",
    "\n",
    "- Similar computation speed and power as Scala\n",
    "\n",
    "- PySpark APIs are similar to Pandas and Scikit-learn\n",
    "\n",
    "- The high level components of a Spark application include the Spark driver, the Spark executors and the Cluster Manager.\n",
    "\n",
    "- Spark supports three cluster managers:\n",
    "\n",
    "    - Built-in standalone cluster managers\n",
    "    - Apache Mesos\n",
    "    - Hadoop YARN\n",
    "\n",
    "- Execution modes:\n",
    "\n",
    "    - Cluster Mode, \n",
    "    - Client Mode(default),\n",
    "    - Local Mode.\n",
    "\n",
    "### What is Spark shell?\n",
    "\n",
    "- Interactive environment for running Spark jobs\n",
    "\n",
    "- Helpful for fast interactive prototyping\n",
    "\n",
    "- Spark’s shells allow interacting with data on disk or in memory\n",
    "\n",
    "- Three different Spark shells:\n",
    "\n",
    "    Spark-shell for Scala\n",
    "\n",
    "    PySpark-shell for Python\n",
    "\n",
    "    SparkR for R\n",
    "\n",
    "\n",
    "### PySpark shell\n",
    "\n",
    "- PySpark shell is the Python-based command line tool\n",
    "\n",
    "- PySpark shell data scientists interfere with Spark data structures\n",
    "\n",
    "- PySpark shell support connecting to cluster\n",
    "\n",
    "\n",
    "### Understanding SparkContext\n",
    "\n",
    "- SparkContext is an entry point into the world of Spark\n",
    "\n",
    "- An entry point is a way of connecting to Spark cluster\n",
    "\n",
    "- An entry point is like a key to the house\n",
    "\n",
    "- PySpark has a default SparkContext called sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import warnings\n",
    "\"\"\"\n",
    "try:\n",
    "    # create SparkContext on all CPUs available: in my case I have 4 CPUs on my laptop\n",
    "    sc = SparkContext(appName=\"SDDM\", master='local[*]')\n",
    "    print(\"Just created a SparkContext\")\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SQLContext\")\n",
    "except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "sc = SparkContext(appName=\"SDDM\")\n",
    "\n",
    "sc.setMaster('spark://fs.das3.liacs.nl:7077')#(\"local[*]\")\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\"\"\"\n",
    "\n",
    "sc = SparkContext(appName=\"SDDM\", master= \"local[*]\")\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# ==>> DO NOT FORGET WHNE YOU'RE DONE>> sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n",
      "3.6\n",
      "local[*]\n"
     ]
    }
   ],
   "source": [
    "# Master can be local[*], spark:// , yarn, etc.\n",
    "# SparkContext available as sc, SQLContext available as sqlContext.\n",
    "\n",
    "# Inspecting SparkContext\n",
    "# Version: To retrieve SparkContext version\n",
    "\n",
    "print (sc.version)\n",
    "#2.3.1\n",
    "\n",
    "# Python Version: To retrieve Python version of SparkContext\n",
    "\n",
    "print(sc.pythonVer)\n",
    "#3.6\n",
    "\n",
    "# Master: URL of the cluster or “local” string to run in local mode of SparkContext\n",
    "\n",
    "print (sc.master)\n",
    "\n",
    "#local[*]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark executon plan to show lazy evaluation with Word Count example\n",
    "\n",
    "# https://github.com/tirthajyoti/Spark-with-Python/blob/master/Word_Count.ipynb\n",
    "\n",
    "# SparkContext - number of workers and lazy evaluation¶\n",
    "\n",
    "# https://github.com/tirthajyoti/Spark-with-Python/blob/master/SparkContext_Workers_Lazy_Evaluations.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark RDD (Resillient Distributed Datasets)\n",
    "\n",
    "### Basics of RDD\n",
    "\n",
    "Resilient Distributed Datasets (RDD) is a fundamental data structure of Spark. It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster. RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.\n",
    "\n",
    "Spark makes use of the concept of RDD to achieve faster and efficient MapReduce operations.\n",
    "\n",
    "Formally, an RDD is a read-only, partitioned collection of records. RDDs can be created through deterministic operations on either data on stable storage or other RDDs. RDD is a fault-tolerant collection of elements that can be operated on in parallel.\n",
    "\n",
    "Spark's core data structure is the Resilient Distributed Dataset (RDD). This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. \n",
    "\n",
    "Resilient: Ability to withstand failures\n",
    "\n",
    "Distributed: Spanning across multiple machines\n",
    "\n",
    "Datasets: Collection of partitioned data e.g, Arrays, Tables, Tuples etc.,\n",
    "\n",
    "RDD is \n",
    "- Lazily evaluated (transformations & actions)\n",
    "- Recomputed on node failure \n",
    "- Distributed across the cluster\n",
    "    \n",
    "Transformations (lazy) \n",
    "\n",
    "    map \n",
    "    filter \n",
    "    flatMap \n",
    "    reduceByKey \n",
    "    join \n",
    "    cogroup\n",
    "\n",
    "Actions (eager) \n",
    "\n",
    "    count \n",
    "    reduce \n",
    "    collect \n",
    "    take \n",
    "    saveAsTextFile \n",
    "    saveAsHadoop \n",
    "    countByValue\n",
    "\n",
    "\n",
    "\n",
    "### Creating RDDs\n",
    "\n",
    "There are two ways to create RDDs,\n",
    "\n",
    "parallelizing an existing collection of objects in your driver program,\n",
    "\n",
    "External datasets (referencing a dataset in an external storage system, such as a shared file system, HDFS, HBase, or any data source offering a Hadoop Input Format.)\n",
    "\n",
    "    Files in HDFS\n",
    "\n",
    "    Objects in Amazon S3 bucket\n",
    "\n",
    "    lines in a text file\n",
    "\n",
    "From existing RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# Loading data in PySpark\n",
    "\n",
    "# Parallelized collection (parallelizing)\n",
    "\n",
    "# parallelize() for creating RDDs from python lists\n",
    "\n",
    "numRDD = sc.parallelize([1,2,3,4])\n",
    "\n",
    "helloRDD = sc.parallelize(\"Hello world\")\n",
    "\n",
    "print (type(helloRDD))\n",
    "\n",
    "\n",
    "#SparkContext's parallelize() method\n",
    "\n",
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "\n",
    "# creating RDDs from external datasets\n",
    "# SparkContext's textFile() method\n",
    "\n",
    "rdd2 = sc.textFile(\"example_text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg EBook of Ulysses, by James Joyce',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with almost',\n",
       " 'no restrictions whatsoever. You may copy it, give it away or re-use',\n",
       " 'it under the terms of the Project Gutenberg License included with this',\n",
       " 'eBook or online at www.gutenberg.org',\n",
       " 'Title: Ulysses',\n",
       " 'Author: James Joyce',\n",
       " 'Release Date: August 1, 2008 [EBook #4300]',\n",
       " 'Last Updated: August 17, 2017',\n",
       " 'Language: English']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.filter(lambda x: x!=\"\").collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32710"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32710\n",
      "[0, 54, 0, 71, 67]\n"
     ]
    }
   ],
   "source": [
    "LineLength = rdd2.map(lambda x : len(x))\n",
    "print (LineLength.count())\n",
    "print (LineLength.collect()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file:/Users/vkocaman/Python_Projects/Leiden/Spark/example_text.txt']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = sc.wholeTextFiles(\"example_text.txt\", 8)\n",
    "\n",
    "rdd3.keys().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data from HDFS\n",
    "# hdfs://localhost:9746 filamentData.csv\n",
    "data = sc.textFile('hdfs://localhost:9746/bookData/filamentData.csv',4)\n",
    "data.take(4)\n",
    "\n",
    "# Read a file from HDFS and count the words\n",
    "https://github.com/radanalyticsio/radanalyticsio.github.io/blob/master/assets/pyspark_hdfs_notebook/PySpark_HDFS.ipynb\n",
    "\n",
    "# Reading input from S3 with Apache Spark on OpenShift\n",
    "https://github.com/radanalyticsio/radanalyticsio.github.io/blob/master/assets/s3-source-example/s3-source-example.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a DataFrame based on a table named \"people\"\n",
    "# stored in a MySQL database.\n",
    "url = \\\n",
    "  \"jdbc:mysql://yourIP:yourPort/test?user=yourUsername;password=yourPassword\"\n",
    "df = sqlContext \\\n",
    "  .read \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"url\", url) \\\n",
    "  .option(\"dbtable\", \"people\") \\\n",
    "  .load()\n",
    "\n",
    "# Looks the schema of this DataFrame.\n",
    "df.printSchema()\n",
    "\n",
    "# Counts people by age\n",
    "countsByAge = df.groupBy(\"age\").count()\n",
    "countsByAge.show()\n",
    "\n",
    "# Saves countsByAge to S3 in the JSON format.\n",
    "countsByAge.write.format(\"json\").save(\"s3a://...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save RDD Data to HDFS\n",
    "playData = sc.textFile('/home/muser/bData/shakespearePlays.txt',4)\n",
    "playDataLineLength = playData.map(lambda x : len(x))\n",
    "\n",
    "# Each file has a single data point because our RDD has four partitions.\n",
    "playDataLineLength.saveAsTextFile('hdfs://localhost:9746/savedData/')\n",
    "#  hadoop fs -cat /savedData/part-00000\n",
    "#  hadoop fs -cat /savedData/part-00001\n",
    "#  hadoop fs -cat /savedData/part-00002\n",
    "#  hadoop fs -cat /savedData/part-00003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p', 's', 'r', 'p']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read a CSV File\n",
    "# Writing a Python Function to Parse CSV Lines\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "def parseCSV(csvRow) :\n",
    "    data = StringIO(csvRow)\n",
    "    dataReader = csv.reader(data, lineterminator = '')\n",
    "    return(next(dataReader))\n",
    "\n",
    "csvRow = \"p,s,r,p\"\n",
    "parseCSV(csvRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['year',\n",
       "  'month',\n",
       "  'day',\n",
       "  'dep_time',\n",
       "  'dep_delay',\n",
       "  'arr_time',\n",
       "  'arr_delay',\n",
       "  'carrier',\n",
       "  'tailnum',\n",
       "  'flight',\n",
       "  'origin',\n",
       "  'dest',\n",
       "  'air_time',\n",
       "  'distance',\n",
       "  'hour',\n",
       "  'minute']]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read csv file and Creating a Paired RDD\n",
    "filamentRDD = sc.textFile('flights_small.csv', 4)\n",
    "filamentRDDCSV = filamentRDD.map(parseCSV)\n",
    "filamentRDDCSV.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Understanding Partitioning in PySpark\n",
    "\n",
    "# A partition is a logical division of a large distributed data set\n",
    "\n",
    "# parallelize() method\n",
    "\n",
    "numRDD = sc.parallelize(range(10), numSlices = 3)\n",
    "\n",
    "#textFile() method\n",
    "\n",
    "fileRDD = sc.textFile(\"example_text.txt\", minPartitions = 6)\n",
    "\n",
    "#The number of partitions in an RDD can be found by using getNumPartitions() method\n",
    "\n",
    "print (fileRDD.getNumPartitions())\n",
    "\n",
    "print (numRDD.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### anonymous functions in Python\n",
    "\n",
    "Lambda functions are anonymous functions in Python\n",
    "\n",
    "Very powerful and used in Python. Quite efficient with map() and filter()\n",
    "\n",
    "Lambda functions create functions to be called later similar to def\n",
    "\n",
    "It returns the functions without any name (i.e anonymous)\n",
    "\n",
    "Inline a function definition or to defer execution of a code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Transformations (lazy evaluation)\n",
    "\n",
    "# map() Transformation\n",
    "\n",
    "# map() transformation applies a function to all elements in the RDD\n",
    "\n",
    "RDD = sc.parallelize([1,2,3,4])\n",
    "RDD_map = RDD.map(lambda x: x * x)\n",
    "\n",
    "RDD_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter() Transformation\n",
    "\n",
    "# Filter transformation returns a new RDD with only the elements that pass the condition\n",
    "\n",
    "RDD = sc.parallelize([1,2,3,4])\n",
    "\n",
    "RDD_filter = RDD.filter(lambda x: x > 2)\n",
    "\n",
    "RDD_filter.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatMap() Transformation\n",
    "\n",
    "# flatMap() transformation returns multiple values for each element in the original RDD\n",
    "\n",
    "\"\"\"\n",
    "Why are we using flatMap, rather than map?\n",
    "\n",
    "The reason is that the operation line.split(\" \") generates a list of strings, \n",
    "so had we used map the result would be an RDD of lists of words. Not an RDD of words.\n",
    "\n",
    "The difference between map and flatMap is that the second expects to get a list as the result \n",
    "from the map and it concatenates the lists to form the RDD.\n",
    "\"\"\"\n",
    "\n",
    "RDD = sc.parallelize([\"hello world\", \"how are you\"])\n",
    "\n",
    "RDD_flatmap = RDD.flatMap(lambda x: x.split(\" \"))\n",
    "\n",
    "RDD_flatmap.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with money and indigestion. Because he comes from Oxford. You know,',\n",
       " 'downstairs and touch him for a guinea. He’s stinking with money and',\n",
       " '—Would I make any money by it? Stephen asked.',\n",
       " 'moved over the shells heaped in the cold stone mortar: whelks and money',\n",
       " '—Thank you, sir, Stephen said, gathering the money together with shy',\n",
       " 'don’t know yet what money is. Money is power. When you have lived',\n",
       " 'Shakespeare say? Put but money in thy purse.',\n",
       " '—He knew what money was, Mr Deasy said. He made money. A poet, yes,',\n",
       " 'of the canteen, over the motley slush. Even money Fair Rebel. Ten to one',\n",
       " 'twelve. By the way go easy with that money like a good young imbecile.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# union() Transformation\n",
    "\n",
    "inputRDD = sc.textFile(\"example_text.txt\")\n",
    "\n",
    "money_RDD = inputRDD.filter(lambda x: \"money\" in x.split())\n",
    "biscuit_RDD = inputRDD.filter(lambda x: \"biscuit\" in x.split())\n",
    "combinedRDD =money_RDD.union(biscuit_RDD)\n",
    "\n",
    "combinedRDD.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with money and indigestion. Because he comes from Oxford. You know,',\n",
       " 'downstairs and touch him for a guinea. He’s stinking with money and',\n",
       " '—Would I make any money by it? Stephen asked.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD actions\n",
    "\n",
    "# Operation return a value after running a computation on the RDD\n",
    "\n",
    "# Basic RDD Actions: \n",
    "\n",
    "# collect () : collect() return all the elements of the dataset as an array\n",
    "\n",
    "# take() : take(N) returns an array with the first N elements of the dataset\n",
    "\n",
    "\n",
    "combinedRDD.take(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with money and indigestion. Because he comes from Oxford. You know,'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first(), top() and count() Actions\n",
    "\n",
    "# first() prints the first element of the RDD\n",
    "\n",
    "combinedRDD.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So he went over to the biscuit tin Bob Doran left to see if there was'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinedRDD.collect()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12]\n"
     ]
    }
   ],
   "source": [
    "# Take top elements\n",
    "# This method should only be used if the resulting array is expected\n",
    "# to be small, as all the data is loaded into the driver's memory.\n",
    "\n",
    "# It returns the list sorted in descending order.\n",
    "\n",
    "print (sc.parallelize([10, 4, 2, 12, 3]).top(1))\n",
    "\n",
    "#print (combinedRDD.top(2)) # the first two lines in a descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count() return the number of elements in the RDD\n",
    "\n",
    "combinedRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "8\n",
      "27\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "numbRDD = sc.parallelize([1,2,3,4])\n",
    "\n",
    "# Create map() transformation to cube numbers\n",
    "cubedRDD = numbRDD.map(lambda x: x**3)\n",
    "\n",
    "# Collect the results\n",
    "numbers_all = cubedRDD.collect ()\n",
    "\n",
    "# Print the numbers from numbers_all\n",
    "for numb in numbers_all:\n",
    "    print(numb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 4\n",
      "Partitions structure: [[0, 1], [2, 3, 4], [5, 6], [7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "# glom () - return an RDD created by coalescing all elements within each partition into a list.\n",
    "\n",
    "# https://medium.com/parrot-prediction/partitioning-in-apache-spark-8134ad840b0\n",
    "\n",
    "rdd=sc.parallelize(range(10), 4)\n",
    "\n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 8\n",
      "Partitions structure: [[0], [1], [2], [3, 4], [5], [6], [7], [8, 9]]\n"
     ]
    }
   ],
   "source": [
    "rdd=sc.parallelize(range(10))\n",
    "\n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to pair RDDs in PySpark\n",
    "\n",
    "Real life datasets are usually key/value pairs\n",
    "\n",
    "Each row is a key and maps to one or more values\n",
    "\n",
    "Pair RDD is a special data structure to work with this kind of datasets\n",
    "\n",
    "Pair RDD: Key is the identifier and value is data\n",
    "\n",
    "Creating pair RDDs\n",
    "\n",
    "Two common ways to create pair RDDs\n",
    "\n",
    "    From a list of key-value tuple\n",
    "    From a regular RDD\n",
    "    \n",
    "Get the data into key/value form for paired RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sam', '23'), ('Mary', '34'), ('Peter', '25')]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)]\n",
    "\n",
    "pairRDD_tuple = sc.parallelize(my_tuple)\n",
    "\n",
    "my_list = ['Sam 23', 'Mary 34', 'Peter 25']\n",
    "\n",
    "regularRDD = sc.parallelize(my_list)\n",
    "\n",
    "pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0], s.split(' ')[1]))\n",
    "\n",
    "pairRDD_RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23', '34', '25']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Fetching Values from a Paired RDD\n",
    "pairRDD_RDD_Values = pairRDD_RDD.values()\n",
    "pairRDD_RDD_Values.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sam', 'Mary', 'Peter']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Fetching Keys from a Paired RDD\n",
    "pairRDD_RDD_Keys = pairRDD_RDD.keys()\n",
    "pairRDD_RDD_Keys.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations on pair RDDs\n",
    "\n",
    "All regular transformations work on pair RDD\n",
    "\n",
    "Have to pass functions that operate on tuples rather than on individual elements\n",
    "\n",
    "Examples of paired RDD Transformations\n",
    "\n",
    "reduceByKey(func): Combine values with the same key\n",
    "\n",
    "groupByKey(): Group values with the same key\n",
    "\n",
    "sortByKey(): Return an RDD sorted by the key\n",
    "\n",
    "join(): Join two pair RDDs based on their key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 4, 25, 1]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can use user functions to map on RDD\n",
    "\n",
    "def get_Squares(num):\n",
    "    return num**2\n",
    "\n",
    "numbRDD = sc.parallelize([1,2,3,4,2,5,1])\n",
    "\n",
    "numbRDD.map(get_Squares).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the distinct numbers\n",
    "\n",
    "numbRDD.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Subract\n",
    "\n",
    "numbRDD2 = sc.parallelize([1, 2, 3])\n",
    "\n",
    "numbRDD.subtract(numbRDD2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  intersection\n",
    "\n",
    "numbRDD.intersection(numbRDD2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "10\n",
      "2.5\n",
      "1.25\n",
      "1.118033988749895\n",
      "(count: 4, mean: 2.5, stdev: 1.118033988749895, max: 4.0, min: 1.0)\n",
      "{'count': 4, 'mean': 2.5, 'sum': 10.0, 'min': 1.0, 'max': 4.0, 'stdev': 1.2909944487358056, 'variance': 1.6666666666666667}\n"
     ]
    }
   ],
   "source": [
    "# calculating basic stats\n",
    "\n",
    "numbRDD = sc.parallelize([1,2,3,4,2,5,1])\n",
    "\n",
    "print (numRDD.min())\n",
    "\n",
    "print (numRDD.max())\n",
    "\n",
    "print (numRDD.sum())\n",
    "\n",
    "print (numRDD.mean())\n",
    "\n",
    "print (numRDD.variance())\n",
    "\n",
    "print (numRDD.stdev())\n",
    "\n",
    "print (numRDD.stats())\n",
    "\n",
    "print (numRDD.stats().asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ronaldo', 34), ('Neymar', 22), ('Messi', 47)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey() transformation\n",
    "\n",
    "# reduceByKey() transformation combines values with the same key\n",
    "\n",
    "# It runs parallel operations for each key in the dataset\n",
    "\n",
    "# It is a transformation and not action\n",
    "\n",
    "regularRDD = sc.parallelize([(\"Messi\", 23), (\"Ronaldo\", 34), (\"Neymar\", 22), (\"Messi\", 24)])\n",
    "\n",
    "pairRDD_reducebykey = regularRDD.reduceByKey(lambda x,y : x + y)\n",
    "\n",
    "pairRDD_reducebykey.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(47, 'Messi'), (34, 'Ronaldo'), (22, 'Neymar')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortByKey() transformation\n",
    "\n",
    "# sortByKey() operation orders pair RDD by key\n",
    "\n",
    "#It returns an RDD sorted by key in ascending or descending order\n",
    "\n",
    "pairRDD_reducebykey_rev = pairRDD_reducebykey.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "pairRDD_reducebykey_rev.sortByKey(ascending=False).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR ['CDG']\n",
      "UK ['LHR']\n",
      "US ['JFK', 'SFO']\n"
     ]
    }
   ],
   "source": [
    "# groupByKey() transformation\n",
    "\n",
    "# groupbykey() groups all the values with the same key in the pair RDD\n",
    "\n",
    "airports = [(\"US\", \"JFK\"),(\"UK\", \"LHR\"),(\"FR\", \"CDG\"),(\"US\", \"SFO\")]\n",
    "\n",
    "regularRDD = sc.parallelize(airports)\n",
    "\n",
    "pairRDD_group = regularRDD.groupByKey().collect()\n",
    "\n",
    "for cont, air in pairRDD_group:\n",
    "    print(cont, list(air))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Neymar', (24, 120)), ('Ronaldo', (32, 80)), ('Messi', (34, 100))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join() transformation\n",
    "\n",
    "# join() transformation joins the two pair RDDs based on their key\n",
    "\n",
    "RDD1 = sc.parallelize([(\"Messi\", 34),(\"Ronaldo\", 32),(\"Neymar\", 24)])\n",
    "\n",
    "RDD2 = sc.parallelize([(\"Ronaldo\", 80),(\"Neymar\", 120),(\"Messi\", 100)])\n",
    "\n",
    "RDD1.join(RDD2).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce() action\n",
    "\n",
    "# reduce(func) action is used for aggregating the elements of a regular RDD\n",
    "\n",
    "# The function should be commutative and associative\n",
    "\n",
    "# An example of reduce() action in PySpark\n",
    "\n",
    "x = [1,3,4,6]\n",
    "RDD = sc.parallelize(x)\n",
    "RDD.reduce(lambda x, y : x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 32\r\n",
      "-rw-r--r--  1 vkocaman  staff  0 Jan  3 14:34 _SUCCESS\r\n",
      "-rw-r--r--  1 vkocaman  staff  0 Jan  3 14:34 part-00000\r\n",
      "-rw-r--r--  1 vkocaman  staff  2 Jan  3 14:34 part-00001\r\n",
      "-rw-r--r--  1 vkocaman  staff  0 Jan  3 14:34 part-00002\r\n",
      "-rw-r--r--  1 vkocaman  staff  2 Jan  3 14:34 part-00003\r\n",
      "-rw-r--r--  1 vkocaman  staff  0 Jan  3 14:34 part-00004\r\n",
      "-rw-r--r--  1 vkocaman  staff  2 Jan  3 14:34 part-00005\r\n",
      "-rw-r--r--  1 vkocaman  staff  0 Jan  3 14:34 part-00006\r\n",
      "-rw-r--r--  1 vkocaman  staff  2 Jan  3 14:34 part-00007\r\n"
     ]
    }
   ],
   "source": [
    "# saveAsTextFile() action\n",
    "# saveAsTextFile() action saves RDD into a text file inside a directory with each partition as a separate file\n",
    "\n",
    "RDD.saveAsTextFile(\"tempFile\")\n",
    "\n",
    "! cd tempFile && ls -l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\r\n",
      "-rw-r--r--  1 vkocaman  staff  0 Jan  3 14:35 _SUCCESS\r\n",
      "-rw-r--r--  1 vkocaman  staff  8 Jan  3 14:35 part-00000\r\n"
     ]
    }
   ],
   "source": [
    "# coalesce() method can be used to save RDD as a single text file\n",
    "\n",
    "! rm -r tempFile # we remove the folder at first\n",
    "\n",
    "RDD.coalesce(1).saveAsTextFile(\"tempFile\")\n",
    "\n",
    "! cd tempFile && ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action Operations on pair RDDs\n",
    "\n",
    "RDD actions available for PySpark pair RDDs\n",
    "\n",
    "Pair RDD actions leverage the key-value data\n",
    "\n",
    "Few examples of pair RDD actions include\n",
    "\n",
    "- countByKey()\n",
    "\n",
    "- collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 2\n",
      "b 1\n"
     ]
    }
   ],
   "source": [
    "# countByKey() action\n",
    "\n",
    "# countByKey() only available for type (K, V)\n",
    "\n",
    "# countByKey() action counts the number of elements for each key\n",
    "\n",
    "# Example of countByKey() on a simple list\n",
    "\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "\n",
    "for key, val in rdd.countByKey().items():\n",
    "    print(key, val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 2, 3: 4}"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collectAsMap() action\n",
    "\n",
    "# collectAsMap() return the key-value pairs in the RDD as a dictionary\n",
    "\n",
    "# Example of collectAsMap() on a simple tuple\n",
    "\n",
    "sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13609, 'the'),\n",
       " (10549, ''),\n",
       " (8134, 'of'),\n",
       " (6551, 'and'),\n",
       " (5841, 'a'),\n",
       " (4788, 'to'),\n",
       " (4619, 'in'),\n",
       " (3034, 'his'),\n",
       " (2712, 'he'),\n",
       " (2430, 'I'),\n",
       " (2391, 'with'),\n",
       " (2169, 'that'),\n",
       " (2006, 'was'),\n",
       " (1894, 'on'),\n",
       " (1791, 'for'),\n",
       " (1680, 'it'),\n",
       " (1505, 'her'),\n",
       " (1363, 'you'),\n",
       " (1246, 'is'),\n",
       " (1217, 'at')]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word count example\n",
    "\n",
    "text_file = sc.textFile(\"example_text.txt\")\n",
    "counts_rdd = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# print the word frequencies in descending order\n",
    "\n",
    "counts_rdd.map(lambda x: (x[1], x[0])) \\\n",
    "    .sortByKey(ascending=False)\\\n",
    "    .collect()[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can also be used for compute-intensive tasks. This code estimates π by \"throwing darts\" at a circle. We pick random points in the unit square ((0, 0) to (1,1)) and see how many fall in the unit circle. The fraction should be π / 4, so we use this to get our estimate. So four times this fraction is equal to π.\n",
    "\n",
    "Note: If a circle of radius R is inscribed inside a square with side length 2R, then the area of the circle will be pi*R^2 and the area of the square will be (2R)^2. So the ratio of the area of the circle to the area of the square will be pi/4. This means that, if you pick N points at random inside the square, approximately N*pi/4 of those points should fall inside the circle.\n",
    "\n",
    "The \"Monte Carlo Method\" is a method of solving problems using statistics. Given the probability, P, that an event will occur in certain conditions, a computer can be used to generate those conditions repeatedly. The number of times the event occurs divided by the number of times the conditions are generated should be approximately equal to P.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.135040\n"
     ]
    }
   ],
   "source": [
    "# how to find pi\n",
    "import random\n",
    "\n",
    "def inside_circle(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "NUM_SAMPLES = 100000\n",
    "\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)) \\\n",
    "             .filter(inside_circle).count()\n",
    "\n",
    "print (\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bigrams and word frequencies\n",
    "\n",
    "For a slightly more complicated task, lets look into splitting up sentences from our documents into word bigrams. A bigram is pair of successive tokens in some sequence. We will look at building bigrams from the sequences of words in each sentence, and then try to find the most frequently occuring ones.\n",
    "\n",
    "The first problem is that values in each partition of our initial RDD describe lines from the file rather than sentences. Sentences may be split over multiple lines. The glom() RDD method is used to create a single entry for each document containing the list of all lines, we can then join the lines up, then resplit them into sentences using \".\" as the separator, using flatMap so that every object in our RDD is now a sentence.\n",
    "\n",
    "Now we have isolated each sentence we can split it into a list of words and extract the word bigrams from it. Our new RDD contains tuples containing the word bigram (itself a tuple containing the first and second word) as the first value and the number 1 as the second value.\n",
    "\n",
    "Finally we can apply the same reduceByKey and sort steps that we used in the wordcount example, to count up the bigrams and sort them in order of descending frequency. In reduceByKey the key is not an individual word but a bigram.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1635, ('of', 'the')),\n",
       " (1384, ('in', 'the')),\n",
       " (657, ('on', 'the')),\n",
       " (609, ('to', 'the')),\n",
       " (460, ('and', 'the')),\n",
       " (401, ('of', 'a')),\n",
       " (360, ('at', 'the')),\n",
       " (345, ('for', 'the')),\n",
       " (325, ('from', 'the')),\n",
       " (323, ('with', 'the'))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigrams and word frequencies\n",
    "\n",
    "sentences = sc.textFile(\"example_text.txt\") \\\n",
    "    .glom() \\\n",
    "    .map(lambda x: \" \".join(x)) \\\n",
    "    .flatMap(lambda x: x.split(\".\"))\n",
    "\n",
    "bigrams = sentences.map(lambda x:x.split()) \\\n",
    "    .flatMap(lambda x: [((x[i],x[i+1]),1) for i in range(0,len(x)-1)])\n",
    "\n",
    "freq_bigrams = bigrams.reduceByKey(lambda x,y:x+y) \\\n",
    "    .map(lambda x:(x[1],x[0])) \\\n",
    "    .sortByKey(False)\n",
    "\n",
    "freq_bigrams.take(10)\n",
    "\n",
    "# http://www.mccarroll.net/blog/pyspark2/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’re used to working with Pandas or data frames in R, you’ll have probably also expected to see a header, but there is none. To make your life easier, you will move on from the RDD and convert it to a DataFrame. Dataframes are preferred over RDDs whenever you can use them. Especially when you’re working with Python, the performance of DataFrames is better than RDDs.\n",
    "\n",
    "But what is the difference between the two?\n",
    "\n",
    "You can use RDDs when you want to perform low-level transformations and actions on your unstructured data. This means that you don’t care about imposing a schema while processing or accessing the attributes by name or column. Tying in to what was said before about performance, by using RDDs, you don’t necessarily want the performance benefits that DataFrames can offer for (semi-) structured data. Use RDDs when you want to manipulate the data with functional programming constructs rather than domain specific expressions.\n",
    "\n",
    "To recapitulate, you’ll switch to DataFrames now to use high-level expressions, to perform SQL queries to explore your data further and to gain columnar access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Apache Spark, a DataFrame is a distributed collection of rows under named columns. It is conceptually equivalent to a table in a relational database, an Excel sheet with Column headers, or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. \n",
    "\n",
    "It also shares some common characteristics with RDD:\n",
    "\n",
    "Immutable in nature : We can create DataFrame / RDD once but can’t change it. And we can transform a DataFrame / RDD after applying transformations.\n",
    "\n",
    "Lazy Evaluations: Which means that a task is not executed until an action is performed.\n",
    "\n",
    "Distributed: RDD and DataFrame both are distributed in nature.\n",
    "\n",
    "Advantages of the DataFrame:\n",
    "\n",
    "DataFrames are designed for processing large collection of structured or semi-structured data.\n",
    "\n",
    "Observations in Spark DataFrame are organised under named columns, which helps Apache Spark to understand the schema of a DataFrame. This helps Spark optimize execution plan on these queries.\n",
    "\n",
    "DataFrame in Apache Spark has the ability to handle petabytes of data.\n",
    "\n",
    "DataFrame has a support for wide range of data format and sources.\n",
    "\n",
    "It has API support for different languages like Python, R, Scala, Java.\n",
    "\n",
    "\n",
    "### Using DataFrames\n",
    "\n",
    "The Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs.\n",
    "\n",
    "When you start modifying and combining columns and rows of data, there are many ways to arrive at the same result, but some often take much longer than others. When using RDDs, it's up to the data scientist to figure out the right way to optimize the query, but the DataFrame implementation has much of this optimization built in!\n",
    "\n",
    "To start working with Spark DataFrames, you first have to create a SparkSession object from your SparkContext. You can think of the SparkContext as your connection to the cluster and the SparkSession as your interface with that connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark SQL is a Spark library for structured data. It provides more information about the structure of data and computation\n",
    "\n",
    "DataFrame is immutable distributed collection of data with named columns\n",
    "\n",
    "Designed for processing both structured (e.g relational database) and unstructured data (e.g JSON)\n",
    "\n",
    "Dataframe API is available in Java, Scala, Python, and R\n",
    "\n",
    "DataFrames in PySpark support both SQL queries (SELECT * from table) or expression methods (df.select())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SparkSession - Entry point for DataFrame API\n",
    "\n",
    "SparkContext is the main entry point for creating RDDs\n",
    "\n",
    "SparkSession provides a single point of entry to interact with Spark DataFrames\n",
    "\n",
    "SparkSession is used to create DataFrame, register DataFrames, execute SQL queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = SparkSession.builder.appName('SDDM_2').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating DataFrames in PySpark\n",
    "\n",
    "Two different methods of creating DataFrames in PySpark\n",
    "\n",
    "From existing RDDs using SparkSession's createDataFrame() method\n",
    "\n",
    "From various data sources (CSV, JSON, TXT) using SparkSession's read method\n",
    "\n",
    "Schema controls the data and helps DataFrames to optimize queries\n",
    "\n",
    "Schema provides information about column name, type of data in the column, empty values etc.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame from RDDD\n",
    "\n",
    "iphones_RDD = sc.parallelize([\n",
    "    (\"XS\", 2018, 5.65, 2.79, 6.24),\n",
    "    (\"XR\", 2018, 5.94, 2.98, 6.84),\n",
    "    (\"X10\", 2017, 5.65, 2.79, 6.13),\n",
    "    (\"8Plus\", 2017, 6.23, 3.07, 7.12)\n",
    "])\n",
    "\n",
    "names = [ 'Model',\n",
    "          'Year',\n",
    "          'Height',\n",
    "          'Width',\n",
    "          'Weight'\n",
    "]\n",
    "\n",
    "iphones_df = ss.createDataFrame(iphones_RDD, schema=names)\n",
    "\n",
    "type(iphones_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Angola</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Benin</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Botswana</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Burkina</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Burundi</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cameroon</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cape Verde</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Central African Republic</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chad</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Comoros</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Congo</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Congo, Democratic Republic of</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Djibouti</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Egypt</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Equatorial Guinea</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Eritrea</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Gabon</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Gambia</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Ghana</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Guinea</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Guinea-Bissau</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ivory Coast</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Kenya</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Lesotho</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Liberia</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Libya</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Madagascar</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Malawi</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Saint Lucia</td>\n",
       "      <td>NORTH AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Saint Vincent and the Grenadines</td>\n",
       "      <td>NORTH AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Trinidad and Tobago</td>\n",
       "      <td>NORTH AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>United States</td>\n",
       "      <td>NORTH AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Australia</td>\n",
       "      <td>OCEANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Fiji</td>\n",
       "      <td>OCEANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Kiribati</td>\n",
       "      <td>OCEANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Marshall Islands</td>\n",
       "      <td>OCEANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Micronesia</td>\n",
       "      <td>OCEANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Nauru</td>\n",
       "      <td>OCEANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>OCEANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Palau</td>\n",
       "      <td>OCEANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Papua New Guinea</td>\n",
       "      <td>OCEANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Samoa</td>\n",
       "      <td>OCEANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Solomon Islands</td>\n",
       "      <td>OCEANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Tonga</td>\n",
       "      <td>OCEANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Tuvalu</td>\n",
       "      <td>OCEANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Vanuatu</td>\n",
       "      <td>OCEANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>SOUTH AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Bolivia</td>\n",
       "      <td>SOUTH AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>SOUTH AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Chile</td>\n",
       "      <td>SOUTH AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Colombia</td>\n",
       "      <td>SOUTH AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Ecuador</td>\n",
       "      <td>SOUTH AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Guyana</td>\n",
       "      <td>SOUTH AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Paraguay</td>\n",
       "      <td>SOUTH AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Peru</td>\n",
       "      <td>SOUTH AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Suriname</td>\n",
       "      <td>SOUTH AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Uruguay</td>\n",
       "      <td>SOUTH AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Venezuela</td>\n",
       "      <td>SOUTH AMERICA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Country         Region\n",
       "0                             Algeria         AFRICA\n",
       "1                              Angola         AFRICA\n",
       "2                               Benin         AFRICA\n",
       "3                            Botswana         AFRICA\n",
       "4                             Burkina         AFRICA\n",
       "5                             Burundi         AFRICA\n",
       "6                            Cameroon         AFRICA\n",
       "7                          Cape Verde         AFRICA\n",
       "8            Central African Republic         AFRICA\n",
       "9                                Chad         AFRICA\n",
       "10                            Comoros         AFRICA\n",
       "11                              Congo         AFRICA\n",
       "12      Congo, Democratic Republic of         AFRICA\n",
       "13                           Djibouti         AFRICA\n",
       "14                              Egypt         AFRICA\n",
       "15                  Equatorial Guinea         AFRICA\n",
       "16                            Eritrea         AFRICA\n",
       "17                           Ethiopia         AFRICA\n",
       "18                              Gabon         AFRICA\n",
       "19                             Gambia         AFRICA\n",
       "20                              Ghana         AFRICA\n",
       "21                             Guinea         AFRICA\n",
       "22                      Guinea-Bissau         AFRICA\n",
       "23                        Ivory Coast         AFRICA\n",
       "24                              Kenya         AFRICA\n",
       "25                            Lesotho         AFRICA\n",
       "26                            Liberia         AFRICA\n",
       "27                              Libya         AFRICA\n",
       "28                         Madagascar         AFRICA\n",
       "29                             Malawi         AFRICA\n",
       "..                                ...            ...\n",
       "164                       Saint Lucia  NORTH AMERICA\n",
       "165  Saint Vincent and the Grenadines  NORTH AMERICA\n",
       "166               Trinidad and Tobago  NORTH AMERICA\n",
       "167                     United States  NORTH AMERICA\n",
       "168                         Australia        OCEANIA\n",
       "169                              Fiji        OCEANIA\n",
       "170                          Kiribati        OCEANIA\n",
       "171                  Marshall Islands        OCEANIA\n",
       "172                        Micronesia        OCEANIA\n",
       "173                             Nauru        OCEANIA\n",
       "174                       New Zealand        OCEANIA\n",
       "175                             Palau        OCEANIA\n",
       "176                  Papua New Guinea        OCEANIA\n",
       "177                             Samoa        OCEANIA\n",
       "178                   Solomon Islands        OCEANIA\n",
       "179                             Tonga        OCEANIA\n",
       "180                            Tuvalu        OCEANIA\n",
       "181                           Vanuatu        OCEANIA\n",
       "182                         Argentina  SOUTH AMERICA\n",
       "183                           Bolivia  SOUTH AMERICA\n",
       "184                            Brazil  SOUTH AMERICA\n",
       "185                             Chile  SOUTH AMERICA\n",
       "186                          Colombia  SOUTH AMERICA\n",
       "187                           Ecuador  SOUTH AMERICA\n",
       "188                            Guyana  SOUTH AMERICA\n",
       "189                          Paraguay  SOUTH AMERICA\n",
       "190                              Peru  SOUTH AMERICA\n",
       "191                          Suriname  SOUTH AMERICA\n",
       "192                           Uruguay  SOUTH AMERICA\n",
       "193                         Venezuela  SOUTH AMERICA\n",
       "\n",
       "[194 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wget -q -O - https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2017-06.csv | head -n 5000 > tmp.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from reading a CSV/JSON/TXT\n",
    "\n",
    "df_csv = spark.read.csv(\"people.csv\", \n",
    "                    header=True, inferSchema=True)\n",
    "\n",
    "df_json = spark.read.json(\"people.json\", \n",
    "                    header=True, inferSchema=True)\n",
    "\n",
    "df_txt = spark.read.txt(\"people.txt\", \n",
    "                    header=True, inferSchema=True)\n",
    "\n",
    "# Path to the file and two optional parameters\n",
    "\n",
    "# Two optional parameters\n",
    "\n",
    "# header=True and inferSchema=True\n",
    "\n",
    "# for more information aboput df.persist(StorageLevel.MEMORY_AND_DISK_SER) see below\n",
    "# https://blog.insightdatascience.com/using-jupyter-on-apache-spark-step-by-step-with-a-terabyte-of-reddit-data-ef4d6c13959a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- faa: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- alt: integer (nullable = true)\n",
      " |-- tz: integer (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv = ss.read.csv(\"airports.csv\", \n",
    "                    header=True, inferSchema=True)\n",
    "\n",
    "# printSchema() operation prints the types of columns in the DataFrame\n",
    "\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame operators in PySpark\n",
    "\n",
    "DataFrame operations: Transformations and Actions\n",
    "\n",
    "DataFrame Transformations:\n",
    "\n",
    "select(), filter(), groupby(), orderby(), dropDuplicates() and withColumnRenamed()\n",
    "\n",
    "DataFrame Actions :\n",
    "\n",
    "printSchema(), head(), show(), count(), columns() and describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+-----------+----+---+---+\n",
      "|faa|                name|       lat|        lon| alt| tz|dst|\n",
      "+---+--------------------+----------+-----------+----+---+---+\n",
      "|04G|   Lansdowne Airport|41.1304722|-80.6195833|1044| -5|  A|\n",
      "|06A|Moton Field Munic...|32.4605722|-85.6800278| 264| -5|  A|\n",
      "|06C| Schaumburg Regional|41.9893408|-88.1012428| 801| -6|  A|\n",
      "|06N|     Randall Airport| 41.431912|-74.3915611| 523| -5|  A|\n",
      "|09J|Jekyll Island Air...|31.0744722|-81.4277778|  11| -4|  A|\n",
      "+---+--------------------+----------+-----------+----+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1397"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                name|\n",
      "+--------------------+\n",
      "|   Lansdowne Airport|\n",
      "|Moton Field Munic...|\n",
      "| Schaumburg Regional|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select() and show() operations\n",
    "\n",
    "# select() transformation subsets the columns in the DataFrame\n",
    "\n",
    "df_id_name = df_csv.select('name')\n",
    "\n",
    "# show() action prints first 20 rows in the DataFrame\n",
    "\n",
    "df_id_name.show(3)\n",
    "\n",
    "# only showing top 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+------------+---+---+---+\n",
      "|faa|                name|       lat|         lon|alt| tz|dst|\n",
      "+---+--------------------+----------+------------+---+---+---+\n",
      "|09J|Jekyll Island Air...|31.0744722| -81.4277778| 11| -4|  A|\n",
      "|1RL|Point Roberts Air...|48.9797222|-123.0788889| 10| -7|  A|\n",
      "|369|  Atmautluak Airport| 60.866667| -162.273056| 18|-10|  A|\n",
      "+---+--------------------+----------+------------+---+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter() transformation filters out the rows based on a condition\n",
    "\n",
    "new_df = df_csv.filter(df_csv.alt < 100)\n",
    "\n",
    "new_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+------------+---+---+---+\n",
      "|faa|                name|       lat|         lon|alt| tz|dst|\n",
      "+---+--------------------+----------+------------+---+---+---+\n",
      "|09J|Jekyll Island Air...|31.0744722| -81.4277778| 11| -4|  A|\n",
      "|1RL|Point Roberts Air...|48.9797222|-123.0788889| 10| -7|  A|\n",
      "|369|  Atmautluak Airport| 60.866667| -162.273056| 18|-10|  A|\n",
      "+---+--------------------+----------+------------+---+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can also use brackets (as in Pandas) instead of filter()\n",
    "\n",
    "df_csv[df_csv.alt < 100].show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+-----------+---+---+---+\n",
      "|faa|                name|       lat|        lon|alt| tz|dst|\n",
      "+---+--------------------+----------+-----------+---+---+---+\n",
      "|60J|Ocean Isle Beach ...|33.9085056|-78.4366722| 32| -5|  U|\n",
      "|HHH|         Hilton Head|    32.216|    -80.752| 10| -5|  U|\n",
      "|HNL|       Honolulu Intl| 21.318681|-157.922428| 13|-10|  N|\n",
      "+---+--------------------+----------+-----------+---+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_csv[(df_csv.alt < 100) & (df_csv[\"dst\"] != 'A')].show(3)\n",
    "\n",
    "# df_csv[(df_csv.alt < 100) & (df_csv.dst != 'A')].show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|dst|count|\n",
      "+---+-----+\n",
      "|  U|   45|\n",
      "|  A| 1329|\n",
      "|  N|   23|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby() operation can be used to group a variable\n",
    "\n",
    "df_csv_group = df_csv.groupby('dst')\n",
    "\n",
    "df_csv_group.count().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|dst|count|\n",
      "+---+-----+\n",
      "|  A| 1329|\n",
      "|  N|   23|\n",
      "|  U|   45|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# orderby() operation sorts the DataFrame based one or more columns\n",
    "\n",
    "df_csv_group.count().orderBy('dst').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropDuplicates() removes the duplicate rows of a DataFrame\n",
    "\n",
    "df_no_dup = df_csv.select('alt', 'dst').dropDuplicates()\n",
    "\n",
    "df_no_dup.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+-----------+--------+---+---+\n",
      "|faa|                name|       lat|        lon|altitude| tz|dst|\n",
      "+---+--------------------+----------+-----------+--------+---+---+\n",
      "|04G|   Lansdowne Airport|41.1304722|-80.6195833|    1044| -5|  A|\n",
      "|06A|Moton Field Munic...|32.4605722|-85.6800278|     264| -5|  A|\n",
      "|06C| Schaumburg Regional|41.9893408|-88.1012428|     801| -6|  A|\n",
      "+---+--------------------+----------+-----------+--------+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumnRenamed() renames a column in the DataFrame\n",
    "\n",
    "df_csv_alt = df_csv.withColumnRenamed('alt', 'altitude')\n",
    "\n",
    "df_csv_alt.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new column from existing ones\n",
    "\n",
    "df_csv_alt = df_csv_alt.withColumn('tzxaltitude', df['tz'] * df['altitude']),\n",
    "# we didn't run this cell before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+-----------+--------+---+\n",
      "|faa|                name|       lat|        lon|altitude|dst|\n",
      "+---+--------------------+----------+-----------+--------+---+\n",
      "|04G|   Lansdowne Airport|41.1304722|-80.6195833|    1044|  A|\n",
      "|06A|Moton Field Munic...|32.4605722|-85.6800278|     264|  A|\n",
      "|06C| Schaumburg Regional|41.9893408|-88.1012428|     801|  A|\n",
      "+---+--------------------+----------+-----------+--------+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dropping a column (no axis concept)\n",
    "\n",
    "df_csv_alt= df_csv_alt.drop(\"tz\")\n",
    "\n",
    "df_csv_alt.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['faa', 'name', 'lat', 'lon', 'alt', 'tz', 'dst']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns() operator prints the columns of a DataFrame\n",
    "\n",
    "df_csv.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|               lat|               lon|               alt|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|              1397|              1397|              1397|\n",
      "|   mean| 41.75029635989892|-103.6891285724532|1005.9169649248389|\n",
      "| stddev|10.549872185047212|30.125313702028542|1521.2701426664623|\n",
      "|    min|         19.721375|          -176.646|               -54|\n",
      "|    max|         72.270833|         174.11362|              9078|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe() operation compute summary statistics of numerical columns in the DataFrame\n",
    "\n",
    "df_csv.select('lat', 'lon', 'alt').describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with DataFrames using PySpark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame API vs SQL queries\n",
    "\n",
    "In PySpark You can interact with SparkSQL through DataFrame API and SQL queries\n",
    "\n",
    "The DataFrame API provides a programmatic domain-specific language (DSL) for data\n",
    "\n",
    "DataFrame transformations and actions are easier to construct programmatically\n",
    "\n",
    "SQL queries can be concise and easier to understand and portable\n",
    "\n",
    "The operations on DataFrames can also be done using SQL queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL notes\n",
    "\n",
    "A SQL query returns a table derived from one or more tables contained in a database.\n",
    "\n",
    "Every SQL query is made up of commands that tell the database what you want to do with the data. The two commands that every query has to contain are SELECT and FROM.\n",
    "\n",
    "The SELECT command is followed by the columns you want in the resulting table.\n",
    "\n",
    "The FROM command is followed by the name of the table that contains those columns. The minimal SQL query is:\n",
    "\n",
    "SELECT * FROM my_table;\n",
    "\n",
    "The * selects all columns, so this returns the entire table named my_table.\n",
    "\n",
    "Similar to .withColumn(), you can do column-wise computations within a SELECT statement. For example,\n",
    "\n",
    "SELECT origin, dest, air_time / 60 FROM flights;\n",
    "\n",
    "returns a table with the origin, destination, and duration in hours for each flight.\n",
    "\n",
    "Another commonly used command is WHERE. This command filters the rows of the table based on some logical condition you specify. The resulting table contains the rows where your condition is true. For example, if you had a table of students and grades you could do:\n",
    "\n",
    "SELECT * FROM students\n",
    "WHERE grade = 'A';\n",
    "\n",
    "to select all the columns and the rows containing information about students who got As.\n",
    "\n",
    "\n",
    "Another common database task is aggregation. That is, reducing your data by breaking it into chunks and summarizing each chunk.\n",
    "\n",
    "This is done in SQL using the GROUP BY command. This command breaks your data into groups and applies a function from your SELECT statement to each group.\n",
    "\n",
    "For example, if you wanted to count the number of flights from each of two origin destinations, you could use the query\n",
    "\n",
    "SELECT COUNT(*) FROM flights\n",
    "GROUP BY origin;\n",
    "\n",
    "GROUP BY origin tells SQL that you want the output to have a row for each unique value of the origin column. The SELECT statement selects the values you want to populate each of the columns. Here, we want to COUNT() every row in each of the groups.\n",
    "\n",
    "It's possible to GROUP BY more than one column. When you do this, the resulting table has a row for every combination of the unique values in each column. The following query counts the number of flights from SEA and PDX to every destination airport:\n",
    "\n",
    "SELECT origin, dest, COUNT(*) FROM flights\n",
    "GROUP BY origin, dest;\n",
    "\n",
    "The output will have a row for every combination of the values in origin and dest (i.e. a row listing each origin and destination that a flight flew to). There will also be a column with the COUNT() of all the rows in each group.\n",
    "\n",
    "Another very common data operation is the join. Joins are a whole topic unto themselves, so in this course we'll just look at simple joins. If you'd like to learn more about joins, you can take a look here.\n",
    "\n",
    "A join will combine two different tables along a column that they share. This column is called the key. Examples of keys here include the tailnum and carrier columns from the flights table.\n",
    "\n",
    "For example, suppose that you want to know more information about the plane that flew a flight than just the tail number. This information isn't in the flights table because the same plane flies many different flights over the course of two years, so including this information in every row would result in a lot of duplication. To avoid this, you'd have a second table that has only one row for each plane and whose columns list all the information about the plane, including its tail number. You could call this table planes\n",
    "\n",
    "When you join the flights table to this table of airplane information, you're adding all the columns from the planes table to the flights table. To fill these columns with information, you'll look at the tail number from the flights table and find the matching one in the planes table, and then use that row to fill out all the new columns.\n",
    "\n",
    "Now you'll have a much bigger table than before, but now every row has all information about the plane that flew that flight!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----------+---+----+\n",
      "|                name|       lat|        lon|dst| alt|\n",
      "+--------------------+----------+-----------+---+----+\n",
      "|   Lansdowne Airport|41.1304722|-80.6195833|  A|1044|\n",
      "|Moton Field Munic...|32.4605722|-85.6800278|  A| 264|\n",
      "| Schaumburg Regional|41.9893408|-88.1012428|  A| 801|\n",
      "|     Randall Airport| 41.431912|-74.3915611|  A| 523|\n",
      "|Elizabethton Muni...|36.3712222|-82.1734167|  A|1593|\n",
      "+--------------------+----------+-----------+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Executing SQL Queries\n",
    "\n",
    "# The SparkSession sql() method executes SQL query\n",
    "\n",
    "# sql() method takes a SQL statement as an argument and returns the result as DataFrame\n",
    "\n",
    "df_csv.createOrReplaceTempView(\"table1\")\n",
    "\n",
    "df2 = ss.sql(\"SELECT name, lat, lon, dst, alt FROM table1 WHERE alt > 100\")\n",
    "\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'lat', 'lon', 'dst', 'alt']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|dst|max(alt)|\n",
      "+---+--------+\n",
      "|  U|    6548|\n",
      "|  A|    9078|\n",
      "|  N|    7015|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summarizing and grouping data using SQL queries\n",
    "\n",
    "df2.createOrReplaceTempView(\"table2\")\n",
    "\n",
    "query = 'SELECT dst, max(alt) FROM table2 GROUP BY dst'\n",
    "\n",
    "ss.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='table1', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='table2', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_delay: string (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: string (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- minute: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = ss.read.csv (\"flights_small.csv\",inferSchema=True, header=True)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
      "|2014|    1| 15|    1037|        7|    1352|        2|     WN| N646SW|    48|   PDX| DEN|     121|     991|  10|    37|\n",
      "|2014|    7|  2|     847|       42|    1041|       51|     WN| N422WN|  1520|   PDX| OAK|      90|     543|   8|    47|\n",
      "|2014|    5| 12|    1655|       -5|    1842|      -18|     VX| N361VA|   755|   SEA| SFO|      98|     679|  16|    55|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"flights\")\n",
    "\n",
    "query = \"FROM flights SELECT * LIMIT 10\"\n",
    "\n",
    "# Get the first 10 rows of flights\n",
    "flights10 = ss.sql(query)\n",
    "\n",
    "# Show the results\n",
    "flights10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandafy a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  origin dest    N\n",
      "0    SEA  RNO    8\n",
      "1    SEA  DTW   98\n",
      "2    SEA  CLE    2\n",
      "3    SEA  LAX  450\n",
      "4    PDX  SEA  144\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n",
    "\n",
    "# Run the query\n",
    "flight_counts = ss.sql(query)\n",
    "\n",
    "# Convert the results to a pandas DataFrame\n",
    "pd_counts = flight_counts.toPandas()\n",
    "\n",
    "# Print the head of pd_counts\n",
    "print(pd_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put some Spark in your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='flights', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='table1', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='table2', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create pd_temp\n",
    "pd_temp = pd.DataFrame(np.random.random(10))\n",
    "\n",
    "# Create spark_temp from pd_temp\n",
    "spark_temp = ss.createDataFrame(pd_temp)\n",
    "\n",
    "# Examine the tables in the catalog\n",
    "ss.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='flights', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='table1', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='table2', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add spark_temp to the catalog\n",
    "spark_temp.name = spark_temp.createOrReplaceTempView('temp')\n",
    "\n",
    "# Examine the tables in the catalog again\n",
    "ss.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "|faa|                name|             lat|              lon| alt| tz|dst|\n",
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "|04G|   Lansdowne Airport|      41.1304722|      -80.6195833|1044| -5|  A|\n",
      "|06A|Moton Field Munic...|      32.4605722|      -85.6800278| 264| -5|  A|\n",
      "|06C| Schaumburg Regional|      41.9893408|      -88.1012428| 801| -6|  A|\n",
      "|06N|     Randall Airport|       41.431912|      -74.3915611| 523| -5|  A|\n",
      "|09J|Jekyll Island Air...|      31.0744722|      -81.4277778|  11| -4|  A|\n",
      "|0A9|Elizabethton Muni...|      36.3712222|      -82.1734167|1593| -4|  A|\n",
      "|0G6|Williams County A...|      41.4673056|      -84.5067778| 730| -5|  A|\n",
      "|0G7|Finger Lakes Regi...|      42.8835647|      -76.7812318| 492| -5|  A|\n",
      "|0P2|Shoestring Aviati...|      39.7948244|      -76.6471914|1000| -5|  U|\n",
      "|0S9|Jefferson County ...|      48.0538086|     -122.8106436| 108| -8|  A|\n",
      "|0W3|Harford County Ai...|      39.5668378|      -76.2024028| 409| -5|  A|\n",
      "|10C|  Galt Field Airport|      42.4028889|      -88.3751111| 875| -6|  U|\n",
      "|17G|Port Bucyrus-Craw...|      40.7815556|      -82.9748056|1003| -5|  A|\n",
      "|19A|Jackson County Ai...|      34.1758638|      -83.5615972| 951| -4|  U|\n",
      "|1A3|Martin Campbell F...|      35.0158056|      -84.3468333|1789| -4|  A|\n",
      "|1B9| Mansfield Municipal|      42.0001331|      -71.1967714| 122| -5|  A|\n",
      "|1C9|Frazier Lake Airpark|54.0133333333333|-124.768333333333| 152| -8|  A|\n",
      "|1CS|Clow Internationa...|      41.6959744|      -88.1292306| 670| -6|  U|\n",
      "|1G3|  Kent State Airport|      41.1513889|      -81.4151111|1134| -4|  A|\n",
      "|1OH|     Fortman Airport|      40.5553253|      -84.3866186| 885| -5|  U|\n",
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"airports.csv\"\n",
    "\n",
    "# Read in the airports data\n",
    "airports = ss.read.csv(file_path, header=True)\n",
    "\n",
    "# Show the data\n",
    "airports.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/Users/vkocaman/Python_Projects/Leiden/Spark/spark-warehouse')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = ss.read.csv('flights_small.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'dep_time',\n",
       " 'dep_delay',\n",
       " 'arr_time',\n",
       " 'arr_delay',\n",
       " 'carrier',\n",
       " 'tailnum',\n",
       " 'flight',\n",
       " 'origin',\n",
       " 'dest',\n",
       " 'air_time',\n",
       " 'distance',\n",
       " 'hour',\n",
       " 'minute']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "flights.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|      duration_hrs|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|               2.2|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|               6.0|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|              1.85|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|1.3833333333333333|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|2.1166666666666667|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add new column\n",
    "\n",
    "flights = flights.withColumn('duration_hrs', flights.air_time / 60)\n",
    "\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|duration_hrs|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|         6.0|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|        2.25|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|         3.3|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|duration_hrs|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|         6.0|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|        2.25|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|         3.3|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Filter flights with a SQL string\n",
    "long_flights1 = flights.filter('distance > 1000')\n",
    "\n",
    "# Filter flights with a boolean column\n",
    "long_flights2 = flights.filter(flights.distance > 1000)\n",
    "\n",
    "# Examine the data to check they're equal\n",
    "print(long_flights1.show(3))\n",
    "print(long_flights2.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+\n",
      "|origin|dest|carrier|\n",
      "+------+----+-------+\n",
      "|   SEA| LAX|     VX|\n",
      "|   SEA| HNL|     AS|\n",
      "|   SEA| SFO|     VX|\n",
      "|   PDX| SJC|     WN|\n",
      "|   SEA| BUR|     AS|\n",
      "|   PDX| DEN|     WN|\n",
      "|   PDX| OAK|     WN|\n",
      "|   SEA| SFO|     VX|\n",
      "|   SEA| SAN|     AS|\n",
      "|   SEA| ORD|     AS|\n",
      "+------+----+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n",
      "+------+----+-------+\n",
      "|origin|dest|carrier|\n",
      "+------+----+-------+\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "+------+----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the first set of columns\n",
    "selected1 = flights.select(\"tailnum\",\"origin\", \"dest\")\n",
    "\n",
    "# Select the second set of columns\n",
    "temp = flights.select(flights.origin, flights.dest, flights.carrier)\n",
    "\n",
    "print (temp.show(10))\n",
    "\n",
    "# Define first filter\n",
    "filterA = (flights.origin == \"SEA\")\n",
    "\n",
    "# Define second filter\n",
    "filterB = (flights.dest == \"PDX\")\n",
    "\n",
    "# Filter the data, first by filterA then by filterB\n",
    "selected2 = temp.filter(filterA).filter(filterB)\n",
    "\n",
    "selected2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+------------------+\n",
      "|origin|dest|tailnum|         avg_speed|\n",
      "+------+----+-------+------------------+\n",
      "|   SEA| LAX| N846VA| 433.6363636363636|\n",
      "|   SEA| HNL| N559AS| 446.1666666666667|\n",
      "|   SEA| SFO| N847VA|367.02702702702703|\n",
      "|   PDX| SJC| N360SW| 411.3253012048193|\n",
      "|   SEA| BUR| N612AS| 442.6771653543307|\n",
      "+------+----+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "+------+----+-------+------------------+\n",
      "|origin|dest|tailnum|         avg_speed|\n",
      "+------+----+-------+------------------+\n",
      "|   SEA| LAX| N846VA| 433.6363636363636|\n",
      "|   SEA| HNL| N559AS| 446.1666666666667|\n",
      "|   SEA| SFO| N847VA|367.02702702702703|\n",
      "|   PDX| SJC| N360SW| 411.3253012048193|\n",
      "|   SEA| BUR| N612AS| 442.6771653543307|\n",
      "+------+----+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define avg_speed\n",
    "avg_speed = (flights.distance/(flights.air_time/60)).alias(\"avg_speed\")\n",
    "\n",
    "# Select the correct columns\n",
    "speed1 = flights.select(\"origin\", \"dest\", \"tailnum\", avg_speed)\n",
    "\n",
    "print (speed1.show(5))\n",
    "\n",
    "# Create the same table using a SQL expression\n",
    "speed2 = flights.selectExpr(\"origin\", \"dest\", \"tailnum\", \"distance/(air_time/60) as avg_speed\")\n",
    "\n",
    "print (speed2.show(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casting\n",
    "\n",
    "flights = flights.withColumn(\"distance\", flights.distance.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flights.withColumn(\"air_time\", flights.air_time.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flights.withColumn(\"dep_delay\", flights.dep_delay.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+\n",
      "|summary|          air_time|         distance|         dep_delay|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|  count|              9925|            10000|              9952|\n",
      "|   mean|152.88423173803525|        1208.1516| 6.068629421221865|\n",
      "| stddev|  72.8656286392139|656.8599023464376|28.808608062751805|\n",
      "|    min|              20.0|             93.0|             -19.0|\n",
      "|    max|             409.0|           2724.0|             886.0|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.describe('air_time', 'distance', \"dep_delay\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|min(distance)|\n",
      "+-------------+\n",
      "|        106.0|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|max(air_time)|\n",
      "+-------------+\n",
      "|        409.0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the shortest flight from PDX in terms of distance\n",
    "flights.filter(flights.origin == \"PDX\").groupBy().min(\"distance\").show()\n",
    "\n",
    "# Find the longest flight from SEA in terms of duration\n",
    "flights.filter(flights.origin == \"SEA\").groupBy().max(\"air_time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     avg(air_time)|\n",
      "+------------------+\n",
      "|188.20689655172413|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "| sum(duration_hrs)|\n",
      "+------------------+\n",
      "|25289.600000000126|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average duration of Delta flights\n",
    "flights.filter(flights.carrier == \"DL\")\\\n",
    "       .filter(flights.origin == \"SEA\")\\\n",
    "       .groupBy().avg('air_time')\\\n",
    "       .show()\n",
    "\n",
    "# Total hours in the air\n",
    "flights.withColumn(\"duration_hrs\", flights.air_time/60).groupBy().sum(\"duration_hrs\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|tailnum|count|\n",
      "+-------+-----+\n",
      "| N442AS|   38|\n",
      "| N102UW|    2|\n",
      "| N36472|    4|\n",
      "| N38451|    4|\n",
      "| N73283|    4|\n",
      "| N513UA|    2|\n",
      "| N954WN|    5|\n",
      "| N388DA|    3|\n",
      "| N567AA|    1|\n",
      "| N516UA|    2|\n",
      "| N927DN|    1|\n",
      "| N8322X|    1|\n",
      "| N466SW|    1|\n",
      "|  N6700|    1|\n",
      "| N607AS|   45|\n",
      "| N622SW|    4|\n",
      "| N584AS|   31|\n",
      "| N914WN|    4|\n",
      "| N654AW|    2|\n",
      "| N336NW|    1|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+------------------+\n",
      "|origin|     avg(air_time)|\n",
      "+------+------------------+\n",
      "|   SEA| 160.4361496051259|\n",
      "|   PDX|137.11543248288737|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by tailnum\n",
    "by_plane = flights.groupBy(\"tailnum\")\n",
    "\n",
    "# Number of flights each plane made\n",
    "by_plane.count().show()\n",
    "\n",
    "# Group by origin\n",
    "by_origin = flights.groupBy(\"origin\")\n",
    "\n",
    "# Average duration of flights from PDX and SEA\n",
    "by_origin.avg(\"air_time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+--------------------+\n",
      "|month|dest|      avg(dep_delay)|\n",
      "+-----+----+--------------------+\n",
      "|   11| TUS| -2.3333333333333335|\n",
      "|   11| ANC|   7.529411764705882|\n",
      "|    1| BUR|               -1.45|\n",
      "|    1| PDX| -5.6923076923076925|\n",
      "|    6| SBA|                -2.5|\n",
      "|    5| LAX|-0.15789473684210525|\n",
      "|   10| DTW|                 2.6|\n",
      "|    6| SIT|                -1.0|\n",
      "|   10| DFW|  18.176470588235293|\n",
      "|    3| FAI|                -2.2|\n",
      "|   10| SEA|                -0.8|\n",
      "|    2| TUS| -0.6666666666666666|\n",
      "|   12| OGG|  25.181818181818183|\n",
      "|    9| DFW|   4.066666666666666|\n",
      "|    5| EWR|               14.25|\n",
      "|    3| RDM|                -6.2|\n",
      "|    8| DCA|                 2.6|\n",
      "|    7| ATL|   4.675675675675675|\n",
      "|    4| JFK| 0.07142857142857142|\n",
      "|   10| SNA| -1.1333333333333333|\n",
      "+-----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+----+----------------------+\n",
      "|month|dest|stddev_samp(dep_delay)|\n",
      "+-----+----+----------------------+\n",
      "|   11| TUS|    3.0550504633038935|\n",
      "|   11| ANC|    18.604716401245316|\n",
      "|    1| BUR|     15.22627576540667|\n",
      "|    1| PDX|     5.677214918493858|\n",
      "|    6| SBA|     2.380476142847617|\n",
      "|    5| LAX|     13.36268698685904|\n",
      "|   10| DTW|     5.639148871948674|\n",
      "|    6| SIT|                   NaN|\n",
      "|   10| DFW|     45.53019017606675|\n",
      "|    3| FAI|    3.1144823004794873|\n",
      "|   10| SEA|     18.70523227029577|\n",
      "|    2| TUS|    14.468356276140469|\n",
      "|   12| OGG|     82.64480404939947|\n",
      "|    9| DFW|    21.728629347782924|\n",
      "|    5| EWR|     42.41595968929191|\n",
      "|    3| RDM|      2.16794833886788|\n",
      "|    8| DCA|     9.946523680831074|\n",
      "|    7| ATL|    22.767001039582183|\n",
      "|    4| JFK|     8.156774303176903|\n",
      "|   10| SNA|    13.726234873756304|\n",
      "+-----+----+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Group by month and dest\n",
    "by_month_dest = flights.groupBy(\"month\", \"dest\")\n",
    "\n",
    "# Average departure delay by month and destination\n",
    "by_month_dest.avg(\"dep_delay\").show()\n",
    "\n",
    "# Standard deviation\n",
    "by_month_dest.agg(F.stddev(\"dep_delay\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "|faa|                name|             lat|              lon| alt| tz|dst|\n",
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "|04G|   Lansdowne Airport|      41.1304722|      -80.6195833|1044| -5|  A|\n",
      "|06A|Moton Field Munic...|      32.4605722|      -85.6800278| 264| -5|  A|\n",
      "|06C| Schaumburg Regional|      41.9893408|      -88.1012428| 801| -6|  A|\n",
      "|06N|     Randall Airport|       41.431912|      -74.3915611| 523| -5|  A|\n",
      "|09J|Jekyll Island Air...|      31.0744722|      -81.4277778|  11| -4|  A|\n",
      "|0A9|Elizabethton Muni...|      36.3712222|      -82.1734167|1593| -4|  A|\n",
      "|0G6|Williams County A...|      41.4673056|      -84.5067778| 730| -5|  A|\n",
      "|0G7|Finger Lakes Regi...|      42.8835647|      -76.7812318| 492| -5|  A|\n",
      "|0P2|Shoestring Aviati...|      39.7948244|      -76.6471914|1000| -5|  U|\n",
      "|0S9|Jefferson County ...|      48.0538086|     -122.8106436| 108| -8|  A|\n",
      "|0W3|Harford County Ai...|      39.5668378|      -76.2024028| 409| -5|  A|\n",
      "|10C|  Galt Field Airport|      42.4028889|      -88.3751111| 875| -6|  U|\n",
      "|17G|Port Bucyrus-Craw...|      40.7815556|      -82.9748056|1003| -5|  A|\n",
      "|19A|Jackson County Ai...|      34.1758638|      -83.5615972| 951| -4|  U|\n",
      "|1A3|Martin Campbell F...|      35.0158056|      -84.3468333|1789| -4|  A|\n",
      "|1B9| Mansfield Municipal|      42.0001331|      -71.1967714| 122| -5|  A|\n",
      "|1C9|Frazier Lake Airpark|54.0133333333333|-124.768333333333| 152| -8|  A|\n",
      "|1CS|Clow Internationa...|      41.6959744|      -88.1292306| 670| -6|  U|\n",
      "|1G3|  Kent State Airport|      41.1513889|      -81.4151111|1134| -4|  A|\n",
      "|1OH|     Fortman Airport|      40.5553253|      -84.3866186| 885| -5|  U|\n",
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+------------+------------------+---------+-----------+---+---+---+\n",
      "|dest|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|air_time|distance|hour|minute|duration_hrs|              name|      lat|        lon|alt| tz|dst|\n",
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+------------+------------------+---------+-----------+---+---+---+\n",
      "| LAX|2014|   12|  8|     658|     -7.0|     935|       -5|     VX| N846VA|  1780|   SEA|   132.0|   954.0|   6|    58|         2.2|  Los Angeles Intl|33.942536|-118.408075|126| -8|  A|\n",
      "| HNL|2014|    1| 22|    1040|      5.0|    1505|        5|     AS| N559AS|   851|   SEA|   360.0|  2677.0|  10|    40|         6.0|     Honolulu Intl|21.318681|-157.922428| 13|-10|  N|\n",
      "| SFO|2014|    3|  9|    1443|     -2.0|    1652|        2|     VX| N847VA|   755|   SEA|   111.0|   679.0|  14|    43|        1.85|San Francisco Intl|37.618972|-122.374889| 13| -8|  A|\n",
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+------------+------------------+---------+-----------+---+---+---+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Rename the faa column\n",
    "airports = airports.withColumnRenamed(\"faa\", \"dest\")\n",
    "\n",
    "# Join the DataFrames\n",
    "flights_with_airports = flights.join(airports, on=\"dest\", how=\"leftouter\")\n",
    "\n",
    "# Examine the data again\n",
    "print(flights_with_airports.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+------------------+--------------------+---------+-----------+----+---+---+\n",
      "|dest|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|air_time|distance|hour|minute|      duration_hrs|                name|      lat|        lon| alt| tz|dst|\n",
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+------------------+--------------------+---------+-----------+----+---+---+\n",
      "| LAX|2014|   12|  8|     658|     -7.0|     935|       -5|     VX| N846VA|  1780|   SEA|   132.0|   954.0|   6|    58|               2.2|    Los Angeles Intl|33.942536|-118.408075| 126| -8|  A|\n",
      "| HNL|2014|    1| 22|    1040|      5.0|    1505|        5|     AS| N559AS|   851|   SEA|   360.0|  2677.0|  10|    40|               6.0|       Honolulu Intl|21.318681|-157.922428|  13|-10|  N|\n",
      "| SFO|2014|    3|  9|    1443|     -2.0|    1652|        2|     VX| N847VA|   755|   SEA|   111.0|   679.0|  14|    43|              1.85|  San Francisco Intl|37.618972|-122.374889|  13| -8|  A|\n",
      "| SJC|2014|    4|  9|    1705|     45.0|    1839|       34|     WN| N360SW|   344|   PDX|    83.0|   569.0|  17|     5|1.3833333333333333|Norman Y Mineta S...|  37.3626|-121.929022|  62| -8|  A|\n",
      "| BUR|2014|    3|  9|     754|     -1.0|    1015|        1|     AS| N612AS|   522|   SEA|   127.0|   937.0|   7|    54|2.1166666666666667|            Bob Hope|34.200667|-118.358667| 778| -8|  A|\n",
      "| DEN|2014|    1| 15|    1037|      7.0|    1352|        2|     WN| N646SW|    48|   PDX|   121.0|   991.0|  10|    37|2.0166666666666666|         Denver Intl|39.861656|-104.673178|5431| -7|  A|\n",
      "| OAK|2014|    7|  2|     847|     42.0|    1041|       51|     WN| N422WN|  1520|   PDX|    90.0|   543.0|   8|    47|               1.5|Metropolitan Oakl...|37.721278|-122.220722|   9| -8|  A|\n",
      "| SFO|2014|    5| 12|    1655|     -5.0|    1842|      -18|     VX| N361VA|   755|   SEA|    98.0|   679.0|  16|    55|1.6333333333333333|  San Francisco Intl|37.618972|-122.374889|  13| -8|  A|\n",
      "| SAN|2014|    4| 19|    1236|     -4.0|    1508|       -7|     AS| N309AS|   490|   SEA|   135.0|  1050.0|  12|    36|              2.25|      San Diego Intl|32.733556|-117.189667|  17| -8|  A|\n",
      "| ORD|2014|   11| 19|    1812|     -3.0|    2352|       -4|     AS| N564AS|    26|   SEA|   198.0|  1721.0|  18|    12|               3.3|  Chicago Ohare Intl|41.978603| -87.904842| 668| -6|  A|\n",
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+------------------+--------------------+---------+-----------+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_with_airports.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "a=flights_with_airports.toPandas()\n",
    "a[\"month\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_with_airports.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('196', '242'), ('186', '302'), ('22', '377'), ('244', '51'), ('166', '346')]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "filename=\"/Users/vkocaman/Python_Projects/Leiden/Hadoop/ml-100k/u.data\"\n",
    "\n",
    "movie_rdd=sc.parallelize([ (x[0],x[1]) for x in csv.reader(open(filename,'r'),delimiter='\\t')])\n",
    "\n",
    "movie_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ss.read.csv(\"/Users/vkocaman/Python_Projects/Leiden/Hadoop/ml-100k/data.csv\", inferSchema=True, header=None).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(737, '405'),\n",
       " (685, '655'),\n",
       " (636, '13'),\n",
       " (540, '450'),\n",
       " (518, '276'),\n",
       " (493, '416'),\n",
       " (490, '537'),\n",
       " (484, '303'),\n",
       " (480, '234'),\n",
       " (448, '393')]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word count example\n",
    "\n",
    "counts_rdd = movie_rdd.map(lambda word: (word[0], 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# print the word frequencies in descending order\n",
    "\n",
    "counts_rdd.map(lambda x: (x[1], x[0])) \\\n",
    "    .sortByKey(ascending=False)\\\n",
    "    .collect()[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=[a[0] for a in [ (x[0],x[1]) for x in csv.reader(open(filename,'r'),delimiter='\\t')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "737"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max(Counter(m).values())\n",
    "\n",
    "Counter(m)[\"405\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySaprk UDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "import warnings\n",
    "\n",
    "ss = SparkSession.builder.appName('SDDM').master(\"local[*]\").getOrCreate()\n",
    "\n",
    "\n",
    "df_spark = ss.read.csv(\"employee_email_data_v2.csv\", \n",
    "                    header=True, inferSchema=True)\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "def generate_udf():\n",
    "    \n",
    "    def spark_func(user, tag, hour):\n",
    "\n",
    "        if \"out\" in user:\n",
    "            txt_1 = user.replace(\"out_\",\"\").replace (\"_\",\" \") + \" who is working for another company\"\n",
    "\n",
    "        else:\n",
    "            txt_1 = user.replace(\"_\",\" \") + \" who is working for our company\"\n",
    "\n",
    "        if tag == \"from\":\n",
    "            txt_2 = \" sent an email\"\n",
    "\n",
    "        elif tag == \"to\":\n",
    "            txt_2 = \" received an email\"\n",
    "\n",
    "        if hour > 17 or hour < 9:\n",
    "            txt_3 = \" between 6 pm and 9 am\"\n",
    "        else:\n",
    "            txt_3 = \" during work hours\"\n",
    "\n",
    "        return txt_1 + txt_2 + txt_3\n",
    "    \n",
    "    return f.udf(spark_func, StringType())\n",
    "\n",
    "\n",
    "%%time \n",
    "\n",
    "df_spark = df_spark.withColumn('comment', \n",
    "                   generate_udf()(f.col('user_ids'), f.col('tag'), f.col('hour')))\n",
    "\n",
    "df_spark.collect(),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/pulse/insider-spark-adventure-bar%C4%B1%C5%9F-can-tayiz/\n",
    "    \n",
    "    https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa?gi=165a78ac88a6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark vs Pandas (similar functions)\n",
    "\n",
    "http://localhost:8888/notebooks/Python_Projects/Leiden/Spark/python_spark_ortak_fonksiyonlar.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with machine learning pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = sc.textFile(\"example_text.txt\")\n",
    "wordSeqs = text_file.map(lambda s: [w.lower() for w in s.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "becomes: 0.926846444606781\n",
      "pounds.: 0.9183464050292969\n",
      "fish: 0.9091755747795105\n",
      "was.: 0.8968182802200317\n",
      "poet: 0.8965718746185303\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.feature import Word2Vec\n",
    "\n",
    "#w2v = Word2Vec()\n",
    "#model = w2v.fit(wordSeqs)\n",
    "\n",
    "# find synonyms for a given word\n",
    "synonyms = model.findSynonyms('money', 5)\n",
    "\n",
    "for word, distance in synonyms:\n",
    "    print(\"{}: {}\".format(word, distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/radanalyticsio/workshop-notebook/blob/master/pyspark.ipynb\n",
    "\n",
    "# https://github.com/radanalyticsio/workshop-notebook/blob/master/ml-basics.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence: \"The grass is green .\" - 5 Tokens]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.embeddings import FlairEmbeddings\n",
    "\n",
    "# The sentence objects holds a sentence that we may want to embed or tag\n",
    "from flair.data import Sentence\n",
    "\n",
    "# init embedding\n",
    "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "\n",
    "# create a sentence\n",
    "sentence = Sentence('The grass is green .')\n",
    "\n",
    "# embed words in sentence\n",
    "flair_embedding_forward.embed(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-06 00:03:47,361 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim.vectors.npy not found in cache, downloading to /var/folders/jb/5px3dvgj4bj86ls0lmbswv680000gn/T/tmpx0dfmmbs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160000128/160000128 [00:32<00:00, 4858492.77B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-06 00:04:20,594 copying /var/folders/jb/5px3dvgj4bj86ls0lmbswv680000gn/T/tmpx0dfmmbs to cache at /Users/vkocaman/.flair/embeddings/glove.gensim.vectors.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-06 00:04:20,933 removing temp file /var/folders/jb/5px3dvgj4bj86ls0lmbswv680000gn/T/tmpx0dfmmbs\n",
      "2019-01-06 00:04:21,133 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim not found in cache, downloading to /var/folders/jb/5px3dvgj4bj86ls0lmbswv680000gn/T/tmp3lk7sk4r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21494764/21494764 [00:04<00:00, 4755968.97B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-06 00:04:25,932 copying /var/folders/jb/5px3dvgj4bj86ls0lmbswv680000gn/T/tmp3lk7sk4r to cache at /Users/vkocaman/.flair/embeddings/glove.gensim\n",
      "2019-01-06 00:04:25,978 removing temp file /var/folders/jb/5px3dvgj4bj86ls0lmbswv680000gn/T/tmp3lk7sk4r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-06 00:04:27,565 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-backward-v0.2rc.pt not found in cache, downloading to /var/folders/jb/5px3dvgj4bj86ls0lmbswv680000gn/T/tmpmdb4arf9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72405799/72405799 [00:15<00:00, 4571768.13B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-06 00:04:44,801 copying /var/folders/jb/5px3dvgj4bj86ls0lmbswv680000gn/T/tmpmdb4arf9 to cache at /Users/vkocaman/.flair/embeddings/lm-news-english-backward-v0.2rc.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-06 00:04:44,962 removing temp file /var/folders/jb/5px3dvgj4bj86ls0lmbswv680000gn/T/tmpmdb4arf9\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings\n",
    "\n",
    "# create a StackedEmbedding object that combines glove and forward/backward flair embeddings\n",
    "stacked_embeddings = StackedEmbeddings([\n",
    "                                        WordEmbeddings('glove'), \n",
    "                                        FlairEmbeddings('news-forward'), \n",
    "                                        FlairEmbeddings('news-backward'),\n",
    "                                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "tensor([-3.8194e-02, -2.4487e-01,  7.2812e-01,  ..., -2.5692e-05,\n",
      "        -5.9604e-03, -2.5547e-03])\n",
      "Token: 2 grass\n",
      "tensor([-8.1353e-01,  9.4042e-01, -2.4048e-01,  ..., -6.7730e-05,\n",
      "        -3.0360e-03, -1.3282e-02])\n",
      "Token: 3 is\n",
      "tensor([-0.5426,  0.4148,  1.0322,  ..., -0.0066, -0.0036, -0.0014])\n",
      "Token: 4 green\n",
      "tensor([-6.7907e-01,  3.4908e-01, -2.3984e-01,  ..., -2.2563e-05,\n",
      "        -1.0894e-04, -4.3916e-03])\n",
      "Token: 5 .\n",
      "tensor([-3.3979e-01,  2.0941e-01,  4.6348e-01,  ...,  4.1382e-05,\n",
      "        -4.4364e-04, -2.5425e-02])\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence('The grass is green .')\n",
    "\n",
    "# just embed a sentence using the StackedEmbedding as you would with any single embedding.\n",
    "stacked_embeddings.embed(sentence)\n",
    "\n",
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 Spain\n",
      "tensor([])\n"
     ]
    }
   ],
   "source": [
    "stacked_embeddings.embed(Sentence(\"Spain\"))\n",
    "\n",
    "for token in Sentence(\"Spain\"):\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## additional resources\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=QaoJNXW6SQo\n",
    "(Spark Tutorial For Beginners | Big Data Spark Tutorial | Apache Spark Tutorial | Simplilearn)\n",
    "\n",
    "Querying large dataset with PySpark SQL from S3 on Local Jupyter Notebook\n",
    "https://blog.insightdatascience.com/using-jupyter-on-apache-spark-step-by-step-with-a-terabyte-of-reddit-data-ef4d6c13959a\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/\n",
    "\n",
    "https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\n",
    "\n",
    "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf\n",
    "\n",
    "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_Cheat_Sheet_Python.pdf\n",
    "\n",
    "https://blog.usejournal.com/spark-study-notes-core-concepts-visualized-5256c44e4090\n",
    "\n",
    "https://data-flair.training/blogs/spark-tutorial/\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning\n",
    "\n",
    "https://nbviewer.jupyter.org/github/mepa/sads-pyspark/blob/master/2017-09-14-PySpark-Workshop.slides.html\n",
    "\n",
    "https://towardsdatascience.com/3-methods-for-parallelization-in-spark-6a1a4333b473\n",
    "\n",
    "Spark Web UI\n",
    "https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-webui.html\n",
    "\n",
    "And some more detail about UI >> \n",
    "https://blog.insightdatascience.com/using-jupyter-on-apache-spark-step-by-step-with-a-terabyte-of-reddit-data-ef4d6c13959a\n",
    "\n",
    "https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa\n",
    "\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning\n",
    "\n",
    "https://towardsdatascience.com/how-does-apache-spark-run-on-a-cluster-974ec2731f20\n",
    "\n",
    "https://blog.usejournal.com/spark-study-notes-core-concepts-visualized-5256c44e4090\n",
    "\n",
    "https://stackoverflow.com/questions/32356143/what-does-setmaster-local-mean-in-spark\n",
    "\n",
    "https://techvidvan.com/tutorials/spark-modes-of-deployment/\n",
    "\n",
    "\n",
    "Spark Streaming\n",
    "\n",
    "Test with netcat local data server (https://spark.apache.org/docs/latest/streaming-programming-guide.html)\n",
    "\n",
    "https://engineering.billymob.com/introducing-spark-streaming-c1b8be36c775\n",
    "\n",
    "https://engineering.billymob.com/apache-spark-streaming-kafka-0-10-1f3c29a694cb\n",
    "\n",
    "https://engineering.billymob.com/feature-integrating-kafka-with-spark-streaming-47763f6bcf58\n",
    "\n",
    "https://medium.com/@kass09/spark-streaming-kafka-in-python-a-test-on-local-machine-edd47814746\n",
    "\n",
    "*https://www.rittmanmead.com/blog/2017/01/getting-started-with-spark-streaming-with-python-and-kafka/  (two diff version of application-- windowed vs batch)\n",
    "\n",
    "(Getting+Started+with+Spark+Streaming+with+Python+and+Kafka.ipynb)\n",
    "\n",
    "Part-2 : (write filtered tweets to another kafka topic)\n",
    "https://www.rittmanmead.com/blog/2017/01/data-processing-and-enrichment-in-spark-streaming-with-python-and-kafka/\n",
    "\n",
    "https://medium.com/@mukeshkumar_46704/getting-streaming-data-from-kafka-with-spark-streaming-using-python-9cd0922fa904\n",
    "\n",
    "Full code\n",
    "https://gist.github.com/rmoff/fb033086b285655ffe7f9ff0582dedbf\n",
    "\n",
    "http://tlfvincent.github.io/2016/09/25/kafka-spark-pipeline-part-1/\n",
    "\n",
    "https://www.supergloo.com/fieldnotes/spark-streaming-kafka-example/\n",
    "\n",
    "https://www.opcito.com/blogs/building-a-real-time-data-pipeline-using-spark-streaming-and-kafka/\n",
    "\n",
    "https://www.opcito.com/blogs/data-ingestion-with-hadoop-yarn-spark-and-kafka/\n",
    "\n",
    "\n",
    "Install and run PySpark on Jupyter Notebook at your local machine \n",
    "\n",
    "https://towardsdatascience.com/how-to-use-pyspark-on-your-computer-9c7180075617\n",
    "\n",
    "https://medium.freecodecamp.org/how-to-set-up-pyspark-for-your-jupyter-notebook-7399dd3cb389\n",
    "\n",
    "https://github.com/tirthajyoti/Spark-with-Python\n",
    "\n",
    "Run PySpark with Docker at your local machine\n",
    "\n",
    "https://levelup.gitconnected.com/using-docker-and-pyspark-134cd4cab867\n",
    "\n",
    "https://medium.com/@suci/running-pyspark-on-jupyter-notebook-with-docker-602b18ac4494\n",
    "\n",
    "https://medium.com/@GaryStafford/getting-started-with-pyspark-for-big-data-analytics-using-jupyter-notebooks-and-docker-ba39d2e3d6c7 (including Postgres db)\n",
    "\n",
    "Install and run PySpark on Jupyter Notebook at AWS EC2 \n",
    "\n",
    "https://medium.com/@josemarcialportilla/getting-spark-python-and-jupyter-notebook-running-on-amazon-ec2-dec599e1c297\n",
    "\n",
    "\n",
    "Install and run PySpark on AWS EMR (with Hadoop and Spark pre-installed)\n",
    "\n",
    "https://towardsdatascience.com/end-to-end-distributed-ml-using-aws-emr-apache-spark-pyspark-and-mongodb-tutorial-with-4d1077f68381 \n",
    "\n",
    "https://medium.com/@datitran/quickstart-pyspark-with-anaconda-on-aws-660252b88c9a\n",
    "\n",
    "**https://medium.com/idealo-tech-blog/using-terraform-to-quick-start-pyspark-on-aws-2bc8ce9dcac\n",
    "\n",
    "Install and run PySpark on Jupyter Notebook at GCP DataProc\n",
    "\thttps://towardsdatascience.com/data-science-for-startups-pyspark-1acf51e9d6ba\n",
    "\n",
    "https://cloud.google.com/blog/products/gcp/google-cloud-platform-for-data-scientists-using-jupyter-notebooks-with-apache-spark-on-google-cloud\n",
    "\n",
    "Submit PySpark jobs on GCP DataProc (with Hadoop and Spark pre-installed)\n",
    "\n",
    "https://towardsdatascience.com/step-by-step-tutorial-pyspark-sentiment-analysis-on-google-dataproc-fef9bef46468\n",
    "\n",
    "Run PySpark on DSLab Machines\n",
    "\n",
    "Running Spark clusters on Databricks Community Edition (just let the students know that this is another option.. no need to delve into)\n",
    "\n",
    "Spark MLlib (ML with PySpark)\n",
    "\n",
    "https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35\n",
    "\n",
    "Data cleaning >> https://github.com/radanalyticsio/workshop-notebook/blob/master/workshop.ipynb\n",
    "\n",
    "https://github.com/radanalyticsio/workshop-notebook/blob/master/ml-basics.ipynb\n",
    "\n",
    "Deploying PySpark ML Model on Google Compute Engine as a REST API\n",
    "\n",
    "https://towardsdatascience.com/deploying-pyspark-ml-model-on-google-compute-engine-as-a-rest-api-d69e126b30b1\n",
    "\n",
    "\n",
    "### technicalities of Spark\n",
    "\n",
    "https://spark.apache.org/docs/latest/spark-standalone.html\n",
    "    \n",
    "http://devopspy.com/python/apache-spark-pyspark-centos-rhel/\n",
    "    \n",
    "https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-webui.html\n",
    "    \n",
    "https://medium.com/ymedialabs-innovation/apache-spark-on-a-multi-node-cluster-b75967c8cb2b\n",
    "    \n",
    "https://github.com/jaceklaskowski/mastering-apache-spark-book/blob/master/spark-standalone-example-2-workers-on-1-node-cluster.adoc\n",
    "    \n",
    "http://spark.apache.org/docs/latest/submitting-applications.html\n",
    "        \n",
    "https://docs.anaconda.com/anaconda-scale/howto/spark-basic/\n",
    "    \n",
    "https://www.datacamp.com/community/tutorials/apache-spark-python\n",
    "    \n",
    "https://data-flair.training/blogs/install-apache-spark-multi-node-cluster/\n",
    "    \n",
    "https://www.programcreek.com/2018/11/install-spark-on-ubuntu-standalone-mode/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
